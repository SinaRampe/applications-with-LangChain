{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SinaRampe/applications-with-LangChain/blob/main/Chroma_DB_Multi_pdf_retriever_Langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "RRYSu48huSUW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f55b1308-4678-49b2-8f75-7b665011c26b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/248.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m248.8/248.8 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install langchain openai tiktoken chromadb pypdf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LangChain multi-doc retriever with ChromaDB\n",
        "\n",
        "***New Points***\n",
        "- Multiple Files\n",
        "- ChromaDB\n",
        "- Source info \n",
        "- gpt-3.5-turbo API"
      ],
      "metadata": {
        "id": "7AnZQpL_IZZZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up LangChain \n"
      ],
      "metadata": {
        "id": "HqwsGJDhvAQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\""
      ],
      "metadata": {
        "id": "dNA4TsHpu6OM"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.document_loaders import PyPDFDirectoryLoader"
      ],
      "metadata": {
        "id": "XHVE9uFb3Ajj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load multiple and process documents"
      ],
      "metadata": {
        "id": "9UcQKUId3X2M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loader = PyPDFDirectoryLoader(\"data/\")\n",
        "raw_documents = loader.load()\n",
        "print(f\"loaded {len(raw_documents) } documents\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHGnuXBy_6TU",
        "outputId": "e0338164-9bc6-49b9-b126-60c1522dbe9a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded 466 documents\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#splitting the text into\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "texts = text_splitter.split_documents(raw_documents)"
      ],
      "metadata": {
        "id": "3__nT0D4Fkmg"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlU5AlqY4gwj",
        "outputId": "4264386c-00fb-4fd6-9d78-8db8ab2dabd8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1095"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts[3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bg6-9jwU4ja_",
        "outputId": "aea0daff-ab9c-4cc8-e9f9-e95f83b5b9c2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='Complexity made simple. This is a rare and pr ecious book  about  NLP,\\ntransformers, and t he growing ecosystem around t hem, Hugging Face.\\nWhether these are still buzzwords to you or  you al ready have a solid\\ngrasp of it all, the authors will navigate you w ith hum or, scientific rigor,\\nand pl enty of code examples into the deepest secrets of the coolest\\ntechnology around. F rom “off-the-shelf pretrained” to “from-scratch\\ncustom” models, and f rom performance to missing labels issues, the\\nauthors addr ess practically every real-life struggle of a ML engineer\\nand pr ovide state-of-the-art solutions, making this book  destined to\\ndictate the standar ds in the field for years to come.\\n—Luca Perrozzi, PhD, Data Science and Machine Learning\\nAssociate Manager at Accenture', metadata={'source': 'data/Natural Language Processing with Transformers Building Language Applications with Hugging Face.pdf', 'page': 2})"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## create the DB"
      ],
      "metadata": {
        "id": "YsYsIy8F4cdm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Embed and store the texts\n",
        "# Supplying a persist_directory will store the embeddings on disk\n",
        "persist_directory = 'db'\n",
        "\n",
        "## here we are using OpenAI embeddings but in future we will swap out to local embeddings\n",
        "embedding = OpenAIEmbeddings(disallowed_special=())\n",
        "\n",
        "vectordb = Chroma.from_documents(documents=texts, \n",
        "                                 embedding=embedding,\n",
        "                                 persist_directory=persist_directory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_eTIZwf4Dk2",
        "outputId": "a74b95b8-9a91-448d-c8b4-8566ff08fc4a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:chromadb:Using embedded DuckDB with persistence: data will be stored in: db\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# persiste the db to disk\n",
        "vectordb.persist()\n",
        "vectordb = None"
      ],
      "metadata": {
        "id": "uRfD_Te-47lb"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we can load the persisted database from disk, and use it as normal. \n",
        "vectordb = Chroma(persist_directory=persist_directory, \n",
        "                  embedding_function=embedding)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-h1y_eAHmD-",
        "outputId": "5503cb18-d7a3-442a-9cd0-1ea6826f6507"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:chromadb:Using embedded DuckDB with persistence: data will be stored in: db\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make a retriever"
      ],
      "metadata": {
        "id": "siLXR-XT0JoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectordb.as_retriever()"
      ],
      "metadata": {
        "id": "6ObunFU30Lxh"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = retriever.get_relevant_documents(\"What is NLP?\")"
      ],
      "metadata": {
        "id": "cYA-H59u0Skn"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0iAuh_B0ZjE",
        "outputId": "ddff4762-c443-4e3d-e318-dc61ca12c6cf"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectordb.as_retriever(search_kwargs={\"k\": 2})"
      ],
      "metadata": {
        "id": "jVWgPJXs1yRq"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever.search_type"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "H4N0IhRM0hHL",
        "outputId": "7b026f5f-68d1-46d5-bdda-fa89d930b0f3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'similarity'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriever.search_kwargs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jXL9u-u0prF",
        "outputId": "a39de387-32fb-4b97-a51a-465fbefb483e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'k': 2}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make a chain"
      ],
      "metadata": {
        "id": "4Ia-4OXa5IeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create the chain to answer questions \n",
        "qa_chain = RetrievalQA.from_chain_type(llm=OpenAI(), \n",
        "                                  chain_type=\"stuff\", \n",
        "                                  retriever=retriever, \n",
        "                                  return_source_documents=True)"
      ],
      "metadata": {
        "id": "MGx8XblM4shW"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Cite sources\n",
        "def process_llm_response(llm_response):\n",
        "    print(llm_response['result'])\n",
        "    print('\\n\\nSources:')\n",
        "    for source in llm_response[\"source_documents\"]:\n",
        "        print(source.metadata['source'])"
      ],
      "metadata": {
        "id": "LZEo26mw8e5k"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# full example\n",
        "query = \"What is NLP?\"\n",
        "llm_response = qa_chain(query)\n",
        "process_llm_response(llm_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKfX4vX-5RFT",
        "outputId": "7b1bd79b-3c40-41da-c4f8-84ca56e4a68b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " NLP stands for natural language processing, which is the use of algorithms to process and understand natural language. It encompasses the whole realm of tasks such as text classification, summarization, translation, question answering, chatbots, natural language understanding, and more.\n",
            "\n",
            "\n",
            "Sources:\n",
            "data/Natural Language Processing with Transformers Building Language Applications with Hugging Face.pdf\n",
            "data/Natural Language Processing with Transformers Building Language Applications with Hugging Face.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# break it down\n",
        "query = \"What is NLP?\"\n",
        "llm_response = qa_chain(query)\n",
        "# process_llm_response(llm_response)\n",
        "llm_response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olRm73t3rNt2",
        "outputId": "aed9722a-cd7e-416a-a83f-360fb38daf52"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'query': 'What is NLP?',\n",
              " 'result': ' NLP stands for natural language processing, and is the application of technology to the interpretation and manipulation of natural language. It encompasses the whole realm of tasks from text classification to summarization, translation, question answering, chatbots, natural language understanding (NLU), and more.',\n",
              " 'source_documents': [Document(page_content='Copilot system is helping me write these lines: you’ll never know how much\\nI really wrote.\\nThe revolution goes far beyond text generation. It encompasses the whole\\nrealm of natural language processing (NLP), from text classification to\\nsummarization, translation, question answering, chatbots, natural language\\nunderstanding (NLU), and more. Wherever there’s language, speech or text,\\nthere’s an application for NLP. You can already ask your phone for\\ntomorrow’s weather, or chat with a virtual help desk assistant to troubleshoot\\na problem, or get meaningful results from search engines that seem to truly1', metadata={'source': 'data/Natural Language Processing with Transformers Building Language Applications with Hugging Face.pdf', 'page': 6}),\n",
              "  Document(page_content='with No Labeled Data\\nNLP (natural language processing), transfer learning in, Transfer Learning in\\nNLP-Hugging Face Transformers: Bridging the Gap\\nNlpAug library, Data Augmentation\\nNLU (natural language understanding), The Encoder Branch\\nnoise, filtering, Creating a dataset with Google BigQuery\\nnonlocal keyword, Training a Tokenizer', metadata={'source': 'data/Natural Language Processing with Transformers Building Language Applications with Hugging Face.pdf', 'page': 439})]}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is Huggingface\"\n",
        "llm_response = qa_chain(query)\n",
        "process_llm_response(llm_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wg-e6fh6rNwz",
        "outputId": "73a59b20-5940-4efe-dede-63aa9eecb122"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Huggingface is an ecosystem of tools and resources that helps developers quickly and easily create models for natural language processing (NLP) tasks. It hosts over 20,000 freely available models and provides filters for tasks, frameworks, datasets, and more.\n",
            "\n",
            "\n",
            "Sources:\n",
            "data/Natural Language Processing with Transformers Building Language Applications with Hugging Face.pdf\n",
            "data/Natural Language Processing with Transformers Building Language Applications with Hugging Face.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is generative ai?\"\n",
        "llm_response = qa_chain(query)\n",
        "process_llm_response(llm_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5KETxphrN3d",
        "outputId": "06f8b70e-3c52-4e4e-dcfa-fbc070d3856e"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Generative AI refers to AI technologies that are able to create content, such as text, images, and audio. It typically involves using deep learning models, such as transformers, to generate new content based on a given prompt.\n",
            "\n",
            "\n",
            "Sources:\n",
            "data/Natural Language Processing with Transformers Building Language Applications with Hugging Face.pdf\n",
            "data/Natural Language Processing with Transformers Building Language Applications with Hugging Face.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qa_chain.retriever.search_type , qa_chain.retriever.vectorstore"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPIhZWAR5n3X",
        "outputId": "548abbf7-a01c-4ab3-9049-bcc0872f64c7"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('similarity', <langchain.vectorstores.chroma.Chroma at 0x7f799c2b3640>)"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(qa_chain.combine_documents_chain.llm_chain.prompt.template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_lp0_796P_-",
        "outputId": "9f5f0b62-b1c0-4836-ca23-a8eaa6327314"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "\n",
            "{context}\n",
            "\n",
            "Question: {question}\n",
            "Helpful Answer:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deleteing the DB"
      ],
      "metadata": {
        "id": "SSxVCnNi5h1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r db.zip ./db"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7xmepGJ2GAE",
        "outputId": "be5a2563-157b-4692-d296-574a15406224"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: db/ (stored 0%)\n",
            "  adding: db/chroma-embeddings.parquet (deflated 29%)\n",
            "  adding: db/index/ (stored 0%)\n",
            "  adding: db/index/id_to_uuid_66206206-1387-4ea7-a77f-7126acab2376.pkl (deflated 36%)\n",
            "  adding: db/index/index_metadata_66206206-1387-4ea7-a77f-7126acab2376.pkl (deflated 5%)\n",
            "  adding: db/index/uuid_to_id_66206206-1387-4ea7-a77f-7126acab2376.pkl (deflated 39%)\n",
            "  adding: db/index/index_66206206-1387-4ea7-a77f-7126acab2376.bin (deflated 17%)\n",
            "  adding: db/chroma-collections.parquet (deflated 50%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To cleanup, you can delete the collection\n",
        "vectordb.delete_collection()\n",
        "vectordb.persist()\n",
        "\n",
        "# delete the directory\n",
        "!rm -rf db/"
      ],
      "metadata": {
        "id": "Jl84qGQt5Wu5"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Starting again loading the db\n",
        "\n",
        "restart the runtime"
      ],
      "metadata": {
        "id": "R2r0ZIBPJp-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip db.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pc7CM_mTQAt",
        "outputId": "c840c934-3cac-464c-defb-41d473c51914"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  db.zip\n",
            "   creating: db/\n",
            "  inflating: db/chroma-embeddings.parquet  \n",
            "   creating: db/index/\n",
            "  inflating: db/index/id_to_uuid_66206206-1387-4ea7-a77f-7126acab2376.pkl  \n",
            "  inflating: db/index/index_metadata_66206206-1387-4ea7-a77f-7126acab2376.pkl  \n",
            "  inflating: db/index/uuid_to_id_66206206-1387-4ea7-a77f-7126acab2376.pkl  \n",
            "  inflating: db/index/index_66206206-1387-4ea7-a77f-7126acab2376.bin  \n",
            "  inflating: db/chroma-collections.parquet  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\""
      ],
      "metadata": {
        "id": "us3F8ZKeRiz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.chains import RetrievalQA"
      ],
      "metadata": {
        "id": "qK1nY4PkKYGo"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "persist_directory = 'db'\n",
        "embedding = OpenAIEmbeddings(disallowed_special=())\n",
        "\n",
        "vectordb2 = Chroma(persist_directory=persist_directory, \n",
        "                  embedding_function=embedding,\n",
        "                   )\n",
        "\n",
        "retriever = vectordb2.as_retriever(search_kwargs={\"k\": 2})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "396RyNbS4EXx",
        "outputId": "cc843007-d828-4eb3-acda-f2756e354f39"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:chromadb:Using embedded DuckDB with persistence: data will be stored in: db\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the turbo LLM\n",
        "turbo_llm = ChatOpenAI(\n",
        "    temperature=0,\n",
        "    model_name='gpt-3.5-turbo'\n",
        ")"
      ],
      "metadata": {
        "id": "F3vkSxxYKCZ9"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the chain to answer questions \n",
        "qa_chain = RetrievalQA.from_chain_type(llm=turbo_llm, \n",
        "                                  chain_type=\"stuff\", \n",
        "                                  retriever=retriever, \n",
        "                                  return_source_documents=True)"
      ],
      "metadata": {
        "id": "PsR60NH5KCfj"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Cite sources\n",
        "def process_llm_response(llm_response):\n",
        "    print(llm_response['result'])\n",
        "    print('\\n\\nSources:')\n",
        "    for source in llm_response[\"source_documents\"]:\n",
        "        print(source.metadata['source'])"
      ],
      "metadata": {
        "id": "RWulTG0eKCfk"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# full example\n",
        "query = \"How much money did Pando raise?\"\n",
        "llm_response = qa_chain(query)\n",
        "process_llm_response(llm_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95634465-dee1-4971-a31b-f4543efbeec5",
        "id": "mDp-g2FtKCfk"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There is no information provided about Pando raising money, so I don't know the answer to that question.\n",
            "\n",
            "\n",
            "Sources:\n",
            "data/Natural Language Processing with Transformers Building Language Applications with Hugging Face.pdf\n",
            "data/Natural Language Processing with Transformers Building Language Applications with Hugging Face.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chat prompts"
      ],
      "metadata": {
        "id": "7fPl26c-TbWw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(qa_chain.combine_documents_chain.llm_chain.prompt.messages[0].prompt.template)"
      ],
      "metadata": {
        "id": "wwyuhrpu5XqM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10189dfe-8601-45c2-b357-b38adba5ced9"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use the following pieces of context to answer the users question. \n",
            "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "----------------\n",
            "{context}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(qa_chain.combine_documents_chain.llm_chain.prompt.messages[1].prompt.template)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcWXvSCHRvHO",
        "outputId": "05e417ab-d348-4dbc-b5be-502f6cfc8625"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{question}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "978QWCeJSRdu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}